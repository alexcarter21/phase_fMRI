{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to run stage 2 analysis of the P1490 dataset\n",
    "\n",
    "In this script we take the output of the preprocessing pipeline and run a first-level GLM on it using nilearn (per individual participant).\n",
    "\n",
    "1. Parse the events file to get the onset times and trial types.\n",
    "2. Parse the output_images.txt file to get the paths to the preprocessed images.\n",
    "3. Create a FirstLevelModel object and fit it to the data.\n",
    "4. Optionally compute a simple contrast.\n",
    "5. Optionally produce a report.\n",
    "6. Return a dictionary of relevant output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.image import load_img, threshold_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn.plotting import view_img_on_surf\n",
    "import pandas as pd\n",
    "from preparefMRI_functions import resample_TR\n",
    "print('Done importing libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories etc.\n",
    "pptID = 'R6688' # for this script we run one ppt at a time\n",
    "\n",
    "base_dir = '/scratch/groups/Projects/P1490'\n",
    "data_dir = 'Data' # directory of all participants' data\n",
    "#nifti_dir='nifti' # A subdirectory of the data_dir in each ppt folder\n",
    "nipype_workdir = 'nipype_workdir' # A subdirectory of the data_dir for each ppt\n",
    "\n",
    "fslDir = os.environ['FSLDIR']\n",
    "#fslDir='/raid/toolbox/fsl/'\n",
    "#fslDir='/groups/labs/wadelab/toolbox/fsl6/'\n",
    "fslDataDir = os.path.join(fslDir,'data/standard/')\n",
    "mni_template = os.path.join(fslDataDir,'MNI152_T1_2mm.nii.gz')\n",
    "\n",
    "t1_dir = 'freesurfer' # directory where recon-all T1 data is\n",
    "t1_folder = f'{pptID}_fs7/mri'\n",
    "t1_image = os.path.join(base_dir, t1_dir, t1_folder, 'T1.nii.gz')\n",
    "\n",
    "parfile_dir = 'parfiles' # A subdirectory of the base_dir\n",
    "# In here we expect to find the parfiles for the fMRI runs in .par format : tsv with columns time, event, duration\n",
    "# If duration is not specified, it will be set to 12 seconds\n",
    "all_parfiles=['color_1.par','color_2.par','color_3.par','color_4.par','color_5.par']\n",
    "parfile_to_fmriFiles_indices=[1,2,3,4,5] # Which fmri files go with which parfiles. we need this incase we don't want to use all parfiles.\n",
    "all_event_files = [os.path.join(base_dir,data_dir,pptID,parfile_dir, f) for f in all_parfiles]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def plot_glm_matrices(glm_list):\n",
    "    \"\"\"\n",
    "    This function plots the design matrices for each Generalized Linear Model (GLM) in the given list.\n",
    "    \n",
    "    Parameters:\n",
    "    glm_list (list): A list of GLM objects.\n",
    "    \"\"\"\n",
    "    # Create a single figure to contain all subplots\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Loop through the GLMs and plot their design matrices as subplots\n",
    "    for i, thisG in enumerate(glm_list):\n",
    "        # Extract the design matrix from the current GLM\n",
    "        dm = thisG.design_matrices_[0]\n",
    "    \n",
    "        # Add a subplot for each design matrix in a 2x2 grid of subplots\n",
    "        ax = fig.add_subplot(2, 2, i + 1)  \n",
    "        plot_design_matrix(dm, ax=ax)  # Use nilearn's plot_design_matrix function to plot the design matrix\n",
    "    \n",
    "    # Adjust subplot layout for better visualization\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the single figure with all subplots\n",
    "    plt.show()\n",
    "\n",
    "def parse_event_tsv(events_tsv):\n",
    "    print('Events TSV:', events_tsv)\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    Read columns: time, event, duration\n",
    "    Return a DataFrame in the format Nilearn expects:\n",
    "    columns = [onset, duration, trial_type, ...]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(events_tsv, sep='\\t', header=None)\n",
    "    # Rename columns to match what Nilearn's FirstLevelModel expects\n",
    "    # by default: onset, duration, trial_type\n",
    "\n",
    "    # Set the column names to be 'onset' and 'trial_type'\n",
    "    # add a new column 'duration' with a default value of 12\n",
    "\n",
    "    # Set the first column name to be onset\n",
    "    df.columns = ['onset', 'trial_type']\n",
    "    df['duration'] = 12\n",
    "\n",
    "    print(f\"Unique trial types in {events_tsv}: {df['trial_type'].unique()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of processed files for each ppt is held in output_images.txt\n",
    "\n",
    "These are the locations of the final aligned files from the registration nipype pipeline. Use these for the GLMs.\n",
    "\n",
    "Use nilearn to compute L1 GLMs for each nifti file.\n",
    "\n",
    "Loop through the fmri files and the parfiles and run the GLM for each pair once, right after preprocessing.\n",
    "\n",
    "This has been split below and need to be run seperately or the kernel will crash in YNiC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the output_images.txt file to get a list of the preprocessed images\n",
    "output_images_file = os.path.join(base_dir,data_dir,pptID,nipype_workdir,'output_images.txt')\n",
    "with open(output_images_file, 'r') as f:\n",
    "    fmri_files = [line.strip() for line in f.readlines()]\n",
    "fmri_files.sort()  # Sort the list of files\n",
    "\n",
    "print('FMRI files:', fmri_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the first image to use for viewing later\n",
    "first_img  = nib.load(fmri_files[0])\n",
    "# Print the size of this image: x y z t\n",
    "print('First image shape:', first_img.shape)\n",
    "\n",
    "# calculate a mask to be used in the GLM later\n",
    "mask_img   = compute_epi_mask(first_img)      # same geometry as runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once then can skip\n",
    "# Here we plot the alignment (structural and functional) for each run\n",
    "# each run has the same prescription and transformation, so really only need to do one\n",
    "\n",
    "fmri_data_list=[] \n",
    "\n",
    "#for fmri_file, events_tsv in zip(fmri_files, all_event_files):\n",
    "fmri_file = fmri_files[0]\n",
    "\n",
    "# Make an image of the mean of the fmri_file for visualisation\n",
    "print('FMRI file:', fmri_file)\n",
    "\n",
    "# I have computed a tmean image on the command line as the kernel sometimes crashed on here.\n",
    "#from pathlib import Path\n",
    "#orig_path = Path(fmri_file)\n",
    "#base = orig_path.name.replace(\".nii.gz\", \"\")\n",
    "#tmean_name = base + \"_tmean.nii.gz\"\n",
    "#tmean_path = orig_path.with_name(tmean_name)\n",
    "#fmri_data = load_img(tmean_path)\n",
    "\n",
    "fmri_data = load_img(fmri_file)\n",
    "fmri_data_list.append(fmri_data)\n",
    "\n",
    "# Load in 4D data and average across the 4th dimension (time)\n",
    "mean_img = fmri_data.get_fdata()\n",
    "mean_img = mean_img.mean(axis=3)\n",
    "\n",
    "# Convert the mean image back to a NIfTI image and plot\n",
    "mean_img = nib.Nifti1Image(mean_img, first_img.affine, first_img.header)\n",
    "view = plot_stat_map(mean_img, title='Mean image',  display_mode='ortho', cut_coords=(0, 0, 0), threshold=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a mask of the data\n",
    "mask_img   = compute_epi_mask(fmri_data)\n",
    "view = plot_stat_map(mask_img, title='Mean image',  display_mode='ortho', cut_coords=(0, 0, 0), threshold=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute one GLM per run\n",
    "results = []\n",
    "for img_path, events_tsv in zip(fmri_files, all_event_files):\n",
    "        events = parse_event_tsv(events_tsv)\n",
    "        glm = FirstLevelModel(\n",
    "                mask_img       = mask_img,     \n",
    "                hrf_model      = 'glover',\n",
    "                noise_model    = 'ar1',\n",
    "                high_pass  = 0.01,\n",
    "                smoothing_fwhm = 4,\n",
    "                t_r            = 0.75,\n",
    "                minimize_memory=True,\n",
    "                verbose        = 1)\n",
    "        glm.fit(img_path, events=events)\n",
    "        results.append(glm)\n",
    "\n",
    "# save these results\n",
    "dump(results, os.path.join(base_dir,data_dir,pptID, 'L1_fits.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the results from the joblib file\n",
    "results = load(os.path.join(base_dir,data_dir,pptID,'L1_fits.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fitted models are saved in the results list dictionary under the key 'model'\n",
    "# We can access them like this:\n",
    "# for result in results:\n",
    "#     print(result.design_matrices_)\n",
    "\n",
    "# For each of the models, we can compute a contrast and save it to a file\n",
    "\n",
    "cNames=['LumDiskIn','LumDiskOut','LMDiskIn','LMDiskOut','SDiskIn','SDiskOut','LumGratingIn','LumGratingOut','LMGratingIn','LMGratingOut','SGratingIn','SGratingOut','ITI']\n",
    "\n",
    "# Plot the design matrices\n",
    "for glm in results:\n",
    "   plot_glm_matrices([glm])\n",
    "\n",
    "contrasts=[]\n",
    "for thisG in results:\n",
    "    # Extract the design matrix from the current GLM\n",
    "    dm = thisG.design_matrices_[0]\n",
    "    dm.columns = list(cNames) + list(dm.columns[13:]) \n",
    "\n",
    "    # Assign values for the conditions you want to compare (1 v -1)\n",
    "    v = np.zeros(dm.shape[1])\n",
    "    cond1_idx  = [dm.columns.get_loc(n) for n in\n",
    "            ['LumDiskIn','LumDiskOut','LMDiskIn','LMDiskOut','SDiskIn','SDiskOut','LumGratingIn','LumGratingOut','LMGratingIn','LMGratingOut','SGratingIn','SGratingOut',]]\n",
    "    cond2_idx = [dm.columns.get_loc(n) for n in\n",
    "            ['ITI']]\n",
    "    v[cond1_idx]  =  1\n",
    "    v[cond2_idx] = -1\n",
    "\n",
    "    # Comput contrast\n",
    "    con_img = thisG.compute_contrast(v, output_type='z_score')\n",
    "    contrasts.append(con_img)\n",
    "\n",
    "# Compute the Level 2 stats - the combined stats on a single subject and session\n",
    "print('Computing L2')\n",
    "\n",
    "# Create a second-level model object\n",
    "second_level_model = SecondLevelModel()\n",
    "design_matrix = pd.DataFrame({'intercept': [1] * len(results)})\n",
    "\n",
    "# Fit the model and save\n",
    "second_level_model = second_level_model.fit(contrasts, design_matrix=design_matrix)\n",
    "print('Fitted L2')\n",
    "dump(second_level_model, os.path.join(base_dir,data_dir,pptID,'L2_fits.joblib'))\n",
    "\n",
    "# Specify the contrast: here, a simple one-sample test\n",
    "contrast_matrix = np.eye(design_matrix.shape[1])  # Identity matrix of size of the design matrix\n",
    "z_map = second_level_model.compute_contrast(output_type='z_score')\n",
    "z_map.to_filename(os.path.join(base_dir,data_dir,pptID,'test_z_map.nii.gz'))     \n",
    "\n",
    "# We also need to save out the effect size for a group-level analysis\n",
    "effect_size=second_level_model.compute_contrast(output_type='effect_size')\n",
    "effect_size.to_filename(os.path.join(base_dir,data_dir,pptID,'test_effMap.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot this contrast - this is really just for us, any data presented will be the group data and shown in the next script\n",
    "zmap_clustThresh = threshold_img(z_map, threshold=2.1, cluster_threshold=5)\n",
    "view = view_img(zmap_clustThresh, black_bg=True, threshold=2)\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also display on a surface\n",
    "view = view_img_on_surf(z_map,darkness=.7,threshold='95%', surf_mesh='fsaverage7',\n",
    "        vol_to_surf_kwargs=dict(\n",
    "        radius=3.0, # in mm\n",
    "        interpolation=\"linear\", # 'linear' or 'nearest'\n",
    "        kind=\"ball\",  # 'auto', 'line', 'ball', or 'depth'\n",
    "        n_samples=10  \n",
    "    ))\n",
    "view \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
