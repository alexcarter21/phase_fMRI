{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook to run stage 3 analysis of the P1490 dataset\n",
    "\n",
    "In this script we load the first-level GLMs for each ppt, and compute second level results. We will then compute a group average.\n",
    "\n",
    "1. Load in first-level GLM per ppt\n",
    "2. Compute group-level responses for all ROIs\n",
    "3. View group-beta maps per condition OR contrast conditions for V1 and LGN\n",
    "4. Statistical analysis\n",
    "    - ANOVA per ROI\n",
    "    - post-hoc t-tests comparing conditions\n",
    "    - t-tests > 0\n",
    "5. Plot these results as violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "import pingouin as pg\n",
    "import os\n",
    "from joblib import dump, load\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image\n",
    "from nilearn.reporting import make_glm_report\n",
    "from nilearn.image import load_img, threshold_img\n",
    "from nilearn.glm.contrasts import compute_contrast\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.maskers import NiftiMasker, NiftiLabelsMasker\n",
    "from nilearn.plotting import view_img_on_surf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json, joblib\n",
    "import pandas as pd\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.masking import apply_mask\n",
    "import seaborn as sns\n",
    "\n",
    "print('Done importing libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "\n",
    "RECOMPUTE_DATA = False\n",
    "\n",
    "# subjects\n",
    "all_pptIDs = sorted(['R5619','R6068','R6159','R6376','R6380','R6425','R6502','R6591','R6611','R6619','R6669','R6688','R6694','R6702','R6770','R6804'])\n",
    "nPpt = len(all_pptIDs)\n",
    "\n",
    "# directories\n",
    "base_dir = '/scratch/groups/Projects/P1490'\n",
    "data_dir = 'Data' # directory of all participants' data\n",
    "t1_dir='freesurfer' # directory where recon-all T1 is\n",
    "atlas_path = os.path.join(base_dir,t1_dir,'MNI/mri/tempBens/benson14_eccen_4_10_varea.nii.gz')\n",
    "lgn_path = os.path.join(base_dir,data_dir,'LGN_atlas/9-LGN.nii.gz')\n",
    "group_dir = os.path.join(base_dir, data_dir, \"group_betas\")\n",
    "\n",
    "# condition contrasts\n",
    "conditions = ['LumDiskIn','LumDiskOut','LMDiskIn','LMDiskOut','SconeDiskIn','SconeDiskOut','LumGratingIn','LumGratingOut','LMGratingIn','LMGratingOut','SconeGratingIn','SconeGratingOut','ITI']\n",
    "# we have to use Scone rather than S - S includes 'Disk'\n",
    "\n",
    "# atlas\n",
    "atlas = load_img(atlas_path) # V1\n",
    "atlas_data  = atlas.get_fdata()\n",
    "\n",
    "lgn_atlas = load_img(lgn_path)\n",
    "lgn_atlas_data  = lgn_atlas.get_fdata()\n",
    "\n",
    "roi_names = ['V1','V2','V3','hV4','VO1','VO2','V3B','V3A','LO1','LO2','TO1','TO2'] # 1-12, 0=background\n",
    "nROI = np.arange(len(roi_names))\n",
    "label2name  = {l: f\"ROI{l}\" for l in roi_names}\n",
    "\n",
    "roi_labels = np.unique(atlas_data.astype(int)) \n",
    "roi_labels = roi_labels[roi_labels != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RECOMPUTE_DATA:\n",
    "    # PER-SUBJECT LOOP \n",
    "    subj_beta_imgs = defaultdict(list)   # for later group maps\n",
    "    # save V1 and LGN beta values\n",
    "    all_betas = []             \n",
    "    lgn_all_betas = []\n",
    "\n",
    "    for p, pptID in enumerate(all_pptIDs):\n",
    "        print(f\"\\n[{p+1}/{nPpt}] Processing subject {pptID}\")\n",
    "\n",
    "        subject_betas = []\n",
    "        lgn_subject_betas = []\n",
    "\n",
    "        # load in the preprocessed, within ppt GLM data\n",
    "        models = joblib.load(os.path.join(base_dir, data_dir, pptID, \"L1_fits.joblib\"))\n",
    "        \n",
    "        # ---------- condition regressors ----------\n",
    "        # the first 13 columns are the ones we want (conditions + ISI), so we can ignore the last 7\n",
    "        run_cols = [[str(cond) for cond in model.design_matrices_[0].columns[:13]]\n",
    "                    for model in models]\n",
    "        condition_cols = set(run_cols[0]).intersection(*run_cols[1:])\n",
    "        condition_cols_sorted = sorted(condition_cols, key=lambda x: int(x))\n",
    "        print(\"     condition columns:\", condition_cols_sorted)\n",
    "\n",
    "        # ---------- extract beta maps (per run) ----------\n",
    "        # this creates a dictionary with CONDITION: beta map x5\n",
    "        run_betas = {cond: [] for cond in condition_cols_sorted}\n",
    "\n",
    "        for model in models:\n",
    "            cols = [str(cond) for cond in model.design_matrices_[0].columns]\n",
    "            for cond in condition_cols_sorted:\n",
    "                vec = np.zeros(len(cols))    \n",
    "                vec[cols.index(cond)] = 1.0\n",
    "                beta_img = model.compute_contrast(vec, output_type=\"effect_size\")\n",
    "                run_betas[cond].append(beta_img)\n",
    "\n",
    "        # ---------- average runs - one beta map per regressor ----------\n",
    "        subj_betas = {cond: mean_img(imgs) for cond, imgs in run_betas.items()}\n",
    "        print('beta maps extracted')\n",
    "\n",
    "        # ---------- V1 ----------\n",
    "        # resample atlas to the beta map geometry \n",
    "        ref_img = next(iter(subj_betas.values()))   # use the first beta map as reference\n",
    "        atlas_resampled = image.resample_to_img(atlas, ref_img, interpolation='nearest', force_resample=True)\n",
    "        atlas_resampled_data = np.squeeze(atlas_resampled.get_fdata().astype(int))\n",
    "\n",
    "        # extract ROI means for every condition \n",
    "        sub_dir = os.path.join(base_dir, data_dir, pptID,'raw_betas')\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        for cond, img in subj_betas.items():\n",
    "\n",
    "            beta_data = img.get_fdata()\n",
    "\n",
    "            for roi in roi_labels:\n",
    "\n",
    "                mask = (atlas_resampled_data == roi)\n",
    "                if not mask.any(): # label absent in volume\n",
    "                    continue   \n",
    "                this_data = beta_data[mask]\n",
    "\n",
    "                # here we calculate the mean and remove any voxels that are greater than 2SD away from the mean\n",
    "                mean = this_data.mean()           \n",
    "                std  = this_data.std(ddof=1)    \n",
    "                mask_std = np.abs(this_data - mean) <= 2 * std  # to remove extreme voxels eg blood vessels or outside the mask\n",
    "                cleaned_data = this_data[mask_std]\n",
    "\n",
    "                beta_value = cleaned_data.mean() # average values in this region\n",
    "                beta_dict = dict(subject=pptID, condition=str(cond), roi=roi, beta=beta_value)\n",
    "\n",
    "                subject_betas.append(beta_dict)\n",
    "                all_betas.append(beta_dict)\n",
    "            \n",
    "                np.savetxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{cond}_roi{roi}_all.npy'), this_data, delimiter=',')\n",
    "                np.savetxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{cond}_roi{roi}_clean.npy'), cleaned_data, delimiter=',')                \n",
    "\n",
    "        # ---------- LGN ----------\n",
    "        # resample atlas to the beta map geometry \n",
    "        ref_img = next(iter(subj_betas.values()))   # use the first beta map as reference\n",
    "        atlas_resampled = image.resample_to_img(lgn_atlas, ref_img, interpolation='nearest', force_resample=True)\n",
    "        atlas_resampled_data = np.squeeze(atlas_resampled.get_fdata().astype(int))\n",
    "\n",
    "        # extract ROI means for every condition \n",
    "        sub_dir = os.path.join(base_dir, data_dir, pptID,'lgn_raw_betas')\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        for cond, img in subj_betas.items():\n",
    "\n",
    "            beta_data = img.get_fdata()\n",
    "\n",
    "            mask = (atlas_resampled_data == 1)\n",
    "            if not mask.any(): # label absent in volume\n",
    "                continue   \n",
    "            this_data = beta_data[mask]\n",
    "\n",
    "            # here we calculate the mean and remove any voxels that are greater than 2SD away from the mean\n",
    "            mean = this_data.mean()           \n",
    "            std  = this_data.std(ddof=1)    \n",
    "            mask_std = np.abs(this_data - mean) <= 2 * std  # to remove extreme voxels eg blood vessels or outside the mask\n",
    "            cleaned_data = this_data[mask_std]\n",
    "\n",
    "            beta_value = cleaned_data.mean() # average values in this region\n",
    "            beta_dict = dict(subject=pptID, condition=str(cond), roi='LGN', beta=beta_value)\n",
    "\n",
    "            lgn_subject_betas.append(beta_dict)\n",
    "            lgn_all_betas.append(beta_dict)\n",
    "        \n",
    "            np.savetxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{cond}_roiLGN_all.npy'), this_data, delimiter=',')\n",
    "            np.savetxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{cond}_roiLGN_clean.npy'), cleaned_data, delimiter=',')        \n",
    "\n",
    "        print('LGN extracted')\n",
    "\n",
    "        # ---------- save ----------\n",
    "        for cond, img in subj_betas.items():\n",
    "            nib.save(img, os.path.join(sub_dir, f\"{int(cond):03d}_beta.nii.gz\"))\n",
    "            subj_beta_imgs[cond].append(img)\n",
    "\n",
    "        df = pd.DataFrame(subject_betas)\n",
    "        df.to_csv(os.path.join(sub_dir, \"roi_beta_long.csv\"), index=False)\n",
    "        df_lgn = pd.DataFrame(lgn_subject_betas)\n",
    "        df_lgn.to_csv(os.path.join(sub_dir, \"lgn_beta_long.csv\"), index=False)\n",
    "        print(\"Saved:\", pptID)\n",
    "\n",
    "    # GROUP-LEVEL AVERAGE\n",
    "    # maps \n",
    "    for cond, imgs in subj_beta_imgs.items():\n",
    "        group_img = mean_img(imgs)\n",
    "        nib.save(group_img, os.path.join(group_dir, f\"{int(cond):03d}_beta_mean.nii.gz\"))\n",
    "\n",
    "    # beta values\n",
    "    df = pd.DataFrame(all_betas)\n",
    "    df.to_csv(os.path.join(group_dir, \"roi_beta_long.csv\"), index=False)\n",
    "    df_lgn = pd.DataFrame(lgn_all_betas)\n",
    "    df_lgn.to_csv(os.path.join(group_dir, \"lgn_beta_long.csv\"), index=False)\n",
    "    print(\"Saved: group\")\n",
    "else:\n",
    "    print(f'Loading predefined data')\n",
    "    # Load the csv file into a dataframe\n",
    "    dtype_map = {'subject': 'category', 'condition': 'category', 'roi': 'category'}\n",
    "    df = pd.read_csv(os.path.join(group_dir,\"roi_beta_long.csv\"), dtype=dtype_map)\n",
    "    df_lgn = pd.read_csv(os.path.join(group_dir,\"lgn_beta_long.csv\"), dtype=dtype_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel histograms\n",
    "# just to make sure there are no odd or extreme voxels (eg blood vessels, outside mask etc.)\n",
    "# generally pretty good\n",
    "\n",
    "for pptID in all_pptIDs:\n",
    "\n",
    "    lgn_roi_labels = list(map(str, roi_labels)) + ['LGN']\n",
    "\n",
    "    for roi in lgn_roi_labels:\n",
    "\n",
    "        plt.close()\n",
    "        plt.figure(figsize=(8,12))\n",
    "\n",
    "        for cond in np.arange(1, len(conditions)+1):\n",
    "\n",
    "            n_voxels_rm = 0\n",
    "            total_voxels = 0 \n",
    "\n",
    "            if cond == 13:\n",
    "                i = 100\n",
    "            else:\n",
    "                i = cond\n",
    "\n",
    "            voxels_all = np.loadtxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{i}_roi{roi}_all.npy'), delimiter=',')\n",
    "            voxels_clean = np.loadtxt(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_cond{i}_roi{roi}_clean.npy'), delimiter=',')\n",
    "\n",
    "            voxels_removed = np.size(voxels_all) - np.size(voxels_clean)\n",
    "            n_voxels_rm = n_voxels_rm + voxels_removed\n",
    "            total_voxels = total_voxels + np.size(voxels_all)\n",
    "\n",
    "            np.savetxt(os.path.join(base_dir, data_dir, pptID, f\"raw_betas/voxels/n_voxels_removed_roi{roi}_cond{i}.csv\"), [n_voxels_rm], fmt='%s')\n",
    "            np.savetxt(os.path.join(base_dir, data_dir, pptID, f\"raw_betas/voxels/n_voxels_total_roi{roi}_cond{i}.csv\"), [total_voxels], fmt='%s')\n",
    "\n",
    "            plt.subplot(3,6,cond)\n",
    "\n",
    "            bin_width = 0.25\n",
    "            data_min = min(voxels_all.min(), voxels_clean.min())\n",
    "            data_max = max(voxels_all.max(), voxels_clean.max())\n",
    "            bins = np.arange(data_min, data_max + bin_width, bin_width)\n",
    "\n",
    "            plt.hist(voxels_all, bins=bins, alpha=0.5, color='blue', label='all')\n",
    "            plt.hist(voxels_clean, bins=bins, alpha=0.5, color='red', label='clean')\n",
    "            plt.title(f'{cond}')\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base_dir, data_dir, pptID, f'raw_betas/voxels/voxels_hists_roi{roi}.png'))\n",
    "        #plt.show()\n",
    "\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the average beta maps for a particular condition in 3D volume\n",
    "# the average response is the change from baseline (ITI)\n",
    "cond = 12\n",
    "\n",
    "mean_beta_map = nib.load(os.path.join(group_dir, f\"{cond:03d}_beta_mean.nii.gz\"))\n",
    "iti_beta_map = nib.load(os.path.join(group_dir, \"100_beta_mean.nii.gz\"))\n",
    "\n",
    "mean_beta_data = mean_beta_map.get_fdata()\n",
    "iti_beta_data = iti_beta_map.get_fdata()\n",
    "\n",
    "change_beta_data = mean_beta_data - iti_beta_data\n",
    "\n",
    "change_beta_map = nib.Nifti1Image(change_beta_data, mean_beta_map.affine, mean_beta_map.header)\n",
    "nib.save(change_beta_map, os.path.join(group_dir, f'{cond:03d}_beta_change.nii.gz'))\n",
    "\n",
    "view=view_img(change_beta_map, black_bg=True, threshold=0.2)\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply V1 benson atlas\n",
    "# Calculate the average voxel response in that region (mean and median - but very little difference)\n",
    "# and view the response on 3D volume\n",
    "\n",
    "cond = 3\n",
    "\n",
    "change_beta_map = nib.load(os.path.join(group_dir, f'{cond:03d}_beta_change.nii.gz'))\n",
    "change_beta_data = change_beta_map.get_fdata()\n",
    "\n",
    "atlas_resampled = image.resample_to_img(atlas, change_beta_map, interpolation='nearest', force_resample=True)\n",
    "atlas_resampled_data = np.squeeze(atlas_resampled.get_fdata().astype(int))\n",
    "\n",
    "#mask = np.isin(atlas_resampled_data, roi_labels)           \n",
    "mask = (atlas_resampled_data == 1) \n",
    "\n",
    "masked_data = np.where(mask, change_beta_data, np.nan)   \n",
    "\n",
    "mean_beta = np.nanmean(masked_data)\n",
    "print(f\"Region: V1 — Mean beta: {mean_beta:.4f}\")\n",
    "med_beta = np.nanmedian(masked_data)\n",
    "print(f\"Region: V1 — Med beta: {med_beta:.4f}\")\n",
    "\n",
    "masked_map = nib.Nifti1Image(masked_data, change_beta_map.affine, change_beta_map.header)\n",
    "\n",
    "view=view_img(masked_map, black_bg=True, threshold=0,draw_cross=False)\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above for LGN\n",
    "\n",
    "cond = 3\n",
    "\n",
    "change_beta_map = nib.load(os.path.join(group_dir, f'{cond:03d}_beta_change.nii.gz'))\n",
    "change_beta_data = change_beta_map.get_fdata()\n",
    "\n",
    "atlas_resampled = image.resample_to_img(lgn_atlas, change_beta_map, interpolation='nearest', force_resample=True)\n",
    "atlas_resampled_data = np.squeeze(atlas_resampled.get_fdata().astype(int))\n",
    "\n",
    "mask = (atlas_resampled_data == 1) \n",
    "\n",
    "masked_data = np.where(mask, change_beta_data, np.nan)   \n",
    "\n",
    "mean_beta = np.nanmean(masked_data)\n",
    "print(f\"Region: LGN — Mean beta: {mean_beta:.4f}\")\n",
    "med_beta = np.nanmedian(masked_data)\n",
    "print(f\"Region: LGN — Med beta: {med_beta:.4f}\")\n",
    "\n",
    "masked_map = nib.Nifti1Image(masked_data, change_beta_map.affine, change_beta_map.header)\n",
    "\n",
    "view=view_img(masked_map, black_bg=True, threshold=0,draw_cross=False)\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot contrasts\n",
    "# These are mainly used for visualisation in the thesis, not statistical analysis\n",
    "\n",
    "conditions = ['LumDiskIn','LumDiskOut','LMDiskIn','LMDiskOut','SconeDiskIn','SconeDiskOut','LumGratingIn','LumGratingOut','LMGratingIn','LMGratingOut','SconeGratingIn','SconeGratingOut','ITI']\n",
    "condition_numbers = np.arange(1,13)\n",
    "contrast_maps = [nib.load(os.path.join(group_dir, f'{c:03d}_beta_change.nii.gz')) for c in condition_numbers]\n",
    "\n",
    "# Define filters\n",
    "include_keyword = 'DiskIn' # or None for all conditions\n",
    "c1 = ['Lum']\n",
    "c2 = ['LM','Scone']\n",
    "\n",
    "c1name = ''.join(c1)\n",
    "c2name = ''.join(c2)\n",
    "\n",
    "# group coding\n",
    "group_coding = []\n",
    "for cond in conditions[:12]:  # First 12 conditions only\n",
    "    if include_keyword is None or include_keyword in cond:\n",
    "        if any(k in cond for k in c1):\n",
    "            group_coding.append(1)\n",
    "        elif any(k in cond for k in c2):\n",
    "            group_coding.append(-1)\n",
    "        else:\n",
    "            group_coding.append(0)\n",
    "    else:\n",
    "        group_coding.append(0)\n",
    "\n",
    "# Load all 12 beta maps\n",
    "condition_numbers = np.arange(1, 13)\n",
    "contrast_maps = [nib.load(os.path.join(group_dir, f'{c:03d}_beta_change.nii.gz')) for c in condition_numbers]\n",
    "\n",
    "# Build design matrix\n",
    "design_matrix = pd.DataFrame({'intercept': 1, 'group_diff': group_coding})\n",
    "\n",
    "# Fit the model\n",
    "second_level_model = SecondLevelModel()\n",
    "second_level_model = second_level_model.fit(contrast_maps, design_matrix=design_matrix)\n",
    "\n",
    "# Compute contrast: test 'group_diff' effect only\n",
    "beta_contrast_map = second_level_model.compute_contrast([0, 1])\n",
    "\n",
    "view = view_img(beta_contrast_map, threshold=2)\n",
    "if include_keyword:\n",
    "    view.save_as_html(os.path.join(group_dir,'contrast_maps', f'{c1name}_v_{c2name}_for{include_keyword}_volume.html'))\n",
    "else:\n",
    "    view.save_as_html(os.path.join(group_dir,'contrast_maps', f'{c1name}_v_{c2name}_volume.html'))\n",
    "view\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also display on a surface\n",
    "view = view_img_on_surf(beta_contrast_map,threshold=2, surf_mesh='fsaverage7',black_bg=True, bg_on_data=True,\n",
    "        vol_to_surf_kwargs=dict(\n",
    "        radius=3.0,              # neighbourhood (mm)\n",
    "        interpolation=\"linear\", # 'linear' or 'nearest'\n",
    "        kind=\"ball\",             # 'auto', 'line', 'ball', or 'depth'\n",
    "        n_samples=10             # number of samples to average\n",
    "    )) # Change fsaverage5 to fsaverage if you want to use the higher-res fsaverage mesh\n",
    "if include_keyword:\n",
    "    view.save_as_html(os.path.join(group_dir,'contrast_maps', f'{c1name}_v_{c2name}_for{include_keyword}_surface.html'))\n",
    "else:\n",
    "    view.save_as_html(os.path.join(group_dir,'contrast_maps', f'{c1name}_v_{c2name}_surface.html'))\n",
    "view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for statistical analysis\n",
    "\n",
    "# keys as strings (important for merges that follow)\n",
    "df[\"subject\"]   = df[\"subject\"].astype(str)\n",
    "df[\"condition\"] = df[\"condition\"].astype(str)\n",
    "df[\"roi\"]       = df[\"roi\"].astype(str)\n",
    "\n",
    "# Isolate each subject’s condition 100 (ITI) value and merge it back\n",
    "baseline = (\n",
    "    df.loc[df[\"condition\"] == \"100\", [\"subject\", \"roi\", \"beta\"]]\n",
    "      .rename(columns={\"beta\": \"beta_100\"})\n",
    ")\n",
    "\n",
    "# merge - a new column with the beta 100 value on every row\n",
    "df_final = df.merge(baseline, on=[\"subject\", \"roi\"], how=\"left\")\n",
    "\n",
    "# Compute the difference (beta - beta 100) \n",
    "df_final[\"delta_beta\"] = df_final[\"beta\"] - df_final[\"beta_100\"]\n",
    "\n",
    "df_final.to_csv(os.path.join(group_dir,\"roi_beta_minus_100.csv\"), index=False) \n",
    "\n",
    "print(df_final)\n",
    "\n",
    "\n",
    "\n",
    "# and for LGN\n",
    "# keys as strings (important for merges that follow)\n",
    "df_lgn[\"subject\"]   = df_lgn[\"subject\"].astype(str)\n",
    "df_lgn[\"condition\"] = df_lgn[\"condition\"].astype(str)\n",
    "df_lgn[\"roi\"]       = df_lgn[\"roi\"].astype(str)\n",
    "\n",
    "# Isolate each subject’s condition 100 (ITI) value and merge it back\n",
    "baseline = (\n",
    "    df_lgn.loc[df_lgn[\"condition\"] == \"100\", [\"subject\", \"roi\", \"beta\"]]\n",
    "      .rename(columns={\"beta\": \"beta_100\"})\n",
    ")\n",
    "\n",
    "# merge - a new column with the beta 100 value on every row\n",
    "df_final_lgn = df_lgn.merge(baseline, on=[\"subject\", \"roi\"], how=\"left\")\n",
    "\n",
    "# Compute the difference (beta - beta 100) \n",
    "df_final_lgn[\"delta_beta\"] = df_final_lgn[\"beta\"] - df_final_lgn[\"beta_100\"]\n",
    "\n",
    "df_final_lgn.to_csv(os.path.join(group_dir,\"lgn_beta_minus_100.csv\"), index=False) # I'm removing the ISI response from every other response on a per-subject basis. This just cleans things up.\n",
    "\n",
    "print(df_final_lgn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "aov = AnovaRM(data = df_final,\n",
    "              depvar = 'delta_beta',\n",
    "              subject = 'subject',\n",
    "              within = ['condition', 'roi']).fit()\n",
    "print('delta beta')\n",
    "print(aov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three way ANOVA for each roi\n",
    "\n",
    "df_final['condition'] = df_final['condition'].astype(int)\n",
    "df_with_factors = df_final[df_final['condition'] != 100].copy()\n",
    "\n",
    "df_final_lgn['condition'] = df_final_lgn['condition'].astype(int)\n",
    "df_with_factors_lgn = df_final_lgn[df_final_lgn['condition'] != 100].copy()\n",
    "\n",
    "condition_map = {\n",
    "    1:  'LumDiskIn',\n",
    "    2:  'LumDiskOut',\n",
    "    3:  'LMDiskIn',\n",
    "    4:  'LMDiskOut',\n",
    "    5:  'SconeDiskIn',\n",
    "    6:  'SconeDiskOut',\n",
    "    7:  'LumGratingIn',\n",
    "    8:  'LumGratingOut',\n",
    "    9:  'LMGratingIn',\n",
    "    10: 'LMGratingOut',\n",
    "    11: 'SconeGratingIn',\n",
    "    12: 'SconeGratingOut',\n",
    "}\n",
    "\n",
    "df_with_factors['condition_label'] = df_with_factors['condition'].map(condition_map)\n",
    "df_with_factors[['colour', 'spatial_freq', 'phase']] = df_with_factors['condition_label'].str.extract(r'(Lum|LM|Scone)(Disk|Grating)(In|Out)')\n",
    "\n",
    "df_with_factors_lgn['condition_label'] = df_with_factors_lgn['condition'].map(condition_map)\n",
    "df_with_factors_lgn[['colour', 'spatial_freq', 'phase']] = df_with_factors_lgn['condition_label'].str.extract(r'(Lum|LM|Scone)(Disk|Grating)(In|Out)')\n",
    "\n",
    "df_with_factors['roi'] = df_with_factors['roi'].astype(int)\n",
    "\n",
    "rois = [1, 2, 3, 4]\n",
    "for r in rois:\n",
    "    roi_data = df_with_factors[df_with_factors['roi'] == r]\n",
    "    aov = AnovaRM(data=roi_data,\n",
    "                    depvar='delta_beta',\n",
    "                    subject='subject',\n",
    "                    within=['colour', 'spatial_freq', 'phase']).fit()\n",
    "\n",
    "    print(f\"\\n=== ROI {roi_names[r-1]} ===\")\n",
    "    print(aov.summary())\n",
    "\n",
    "    if r == 1:\n",
    "        post_hoc = pg.pairwise_tests(dv='delta_beta', within='condition', subject='subject', \n",
    "                                    data=roi_data, padjust='fdr_bh',effsize='cohen')\n",
    "        print(post_hoc)\n",
    "        \n",
    "        ttest_results = []\n",
    "        # Loop through each condition\n",
    "        for condition, group in roi_data.groupby('condition'):\n",
    "            ttest = pg.ttest(group['delta_beta'], 0, alternative='greater')  # One-sample, one-tailed\n",
    "            ttest['condition'] = condition\n",
    "            ttest_results.append(ttest)\n",
    "        # Combine all results into one DataFrame\n",
    "        results_df = pd.concat(ttest_results).reset_index(drop=True)\n",
    "        print(results_df[['condition', 'T', 'p-val', 'dof', 'CI95%', 'power']])\n",
    "\n",
    "\n",
    "\n",
    "aov = AnovaRM(data=df_with_factors_lgn,\n",
    "                depvar='delta_beta',\n",
    "                subject='subject',\n",
    "                within=['colour', 'spatial_freq', 'phase']).fit()\n",
    "\n",
    "print(f\"\\n=== ROI LGN ===\")\n",
    "print(aov.summary())\n",
    "\n",
    "post_hoc_lgn = pg.pairwise_tests(dv='delta_beta', within='condition', subject='subject', \n",
    "                            data=df_with_factors_lgn, padjust='fdr_bh',effsize='cohen')\n",
    "print(post_hoc_lgn)\n",
    "\n",
    "ttest_results_lgn = []\n",
    "# Loop through each condition\n",
    "for condition, group in df_with_factors_lgn.groupby('condition'):\n",
    "    ttest = pg.ttest(group['delta_beta'], 0, alternative='greater')  # One-sample, one-tailed\n",
    "    ttest['condition'] = condition\n",
    "    ttest_results_lgn.append(ttest)\n",
    "# Combine all results into one DataFrame\n",
    "results_df_lgn = pd.concat(ttest_results_lgn).reset_index(drop=True)\n",
    "print(results_df_lgn[['condition', 'T', 'p-val', 'dof', 'CI95%', 'power']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and SEM across subjects\n",
    "\n",
    "df_final[\"condition\"] = df_final[\"condition\"].astype(int)\n",
    "df_final[\"roi\"]       = df_final[\"roi\"].astype(int)\n",
    "\n",
    "grouped_beta = df_final.groupby(['condition', 'roi'])['beta']\n",
    "ave_beta = grouped_beta.agg(['mean', lambda x: x.sem()])\n",
    "ave_beta = ave_beta.rename(columns={'<lambda_0>': 'sem'}).reset_index()\n",
    "ave_beta = ave_beta.sort_values(by=['condition', 'roi']).reset_index(drop=True)\n",
    "\n",
    "grouped_delta = df_final.groupby(['condition', 'roi'])['delta_beta']\n",
    "ave_delta = grouped_delta.agg(['mean', lambda x: x.sem()])\n",
    "ave_delta = ave_delta.rename(columns={'<lambda_0>': 'sem'}).reset_index()\n",
    "ave_delta = ave_delta.sort_values(by=['condition', 'roi']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator  \n",
    "\n",
    "dv = 'delta_beta'\n",
    "\n",
    "# violin plots\n",
    "plot_data = df_with_factors[df_with_factors[\"condition\"] != 100]\n",
    "roi_rows = plot_data[plot_data[\"roi\"] == 1]\n",
    "\n",
    "roi_data = df_with_factors[df_with_factors['roi'] == 1]\n",
    "\n",
    "post_hoc = pg.pairwise_tests(dv='delta_beta', within='condition', subject='subject', \n",
    "                                    data=roi_data, padjust='fdr_bh',effsize='cohen')\n",
    "\n",
    "# per chromaticity\n",
    "lum_rows = roi_rows[roi_rows['colour'] == 'Lum']\n",
    "lm_rows = roi_rows[roi_rows['colour'] == 'LM']\n",
    "s_rows = roi_rows[roi_rows['colour'] == 'Scone']\n",
    "\n",
    "# plotting here\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(9, 5),sharey=True)\n",
    "\n",
    "# LUMINANCE\n",
    "palette = sns.color_palette('Greys', n_colors=2); palette.reverse()\n",
    "sns.violinplot(lum_rows, x='condition', y=dv, ax=ax1, \n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax1.set_xticks([0.5, 2.5]); ax1.set_xticklabels(['Disk','Grating'])\n",
    "ax1.set_xlabel(\" \")\n",
    "ax1.set_ylabel(\"Change in beta\")\n",
    "ax1.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax1.legend(title='Phase',loc='lower right')\n",
    "ax1.set_title('Luminance')\n",
    "ax1.spines['top'].set_visible(False); ax1.spines['right'].set_visible(False)\n",
    "ax1.set_ylim([-6, 6])\n",
    "\n",
    "#pairs = list(lum_post_hoc[['A', 'B']].itertuples(index=False, name=None))\n",
    "# We want particular comparisons e.g. disk in phase compared to grating out of phase doesnt really mean anything\n",
    "pairs = [(1, 2), (1, 7), (2, 8), (7, 8)] \n",
    "pvals = list(post_hoc['p-corr'])\n",
    "pvals_wanted = [pvals[i] for i in [0,5,16,51]]\n",
    "annotator = Annotator(ax1, pairs=pairs, data=lum_rows, x='condition', y=dv, verbose=0)\n",
    "annotator.configure(test=None, text_format='star', loc='inside', verbose=0)\n",
    "annotator.set_pvalues_and_annotate(pvals_wanted)\n",
    "\n",
    "# L-M\n",
    "palette = sns.color_palette('Reds', n_colors=2); palette.reverse()\n",
    "sns.violinplot(lm_rows, x='condition', y=dv, ax=ax2,\n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax2.set_xticks([0.5, 2.5]); ax2.set_xticklabels(['Disk','Grating'])\n",
    "ax2.set_xlabel(\"Spatial Frequency\")\n",
    "ax2.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax2.legend(title='Phase',loc='lower right')\n",
    "ax2.set_title('L-M')\n",
    "ax2.spines['top'].set_visible(False); ax2.spines['right'].set_visible(False)\n",
    "ax2.set_ylim([-6, 6])\n",
    "\n",
    "#pairs = list(lm_post_hoc[['A', 'B']].itertuples(index=False, name=None))\n",
    "pairs = [(3, 4), (3, 9), (4, 10), (9, 10)]\n",
    "pvals = list(post_hoc['p-corr'])\n",
    "pvals_wanted = [pvals[i] for i in [21,26,35,60]]\n",
    "annotator = Annotator(ax2, pairs=pairs, data=lm_rows, x='condition', y=dv, verbose=0)\n",
    "annotator.configure(test=None, text_format='star', loc='inside', verbose=0)\n",
    "annotator.set_pvalues_and_annotate(pvals_wanted)\n",
    "\n",
    "# S\n",
    "palette = sns.color_palette('Blues', n_colors=2); palette.reverse()\n",
    "sns.violinplot(s_rows, x='condition', y=dv, ax=ax3,\n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax3.set_xticks([0.5, 2.5]); ax3.set_xticklabels(['Disk','Grating'])\n",
    "ax3.set_xlabel(\" \")\n",
    "ax3.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax3.legend(title='Phase',loc='lower right')\n",
    "ax3.set_title('S-cone')\n",
    "ax3.spines['top'].set_visible(False); ax3.spines['right'].set_visible(False)\n",
    "ax3.set_ylim([-6, 6])\n",
    "\n",
    "#pairs = list(s_post_hoc[['A', 'B']].itertuples(index=False, name=None))\n",
    "pairs = [(5, 6), (5, 11), (6, 12), (11, 12)]\n",
    "pvals = list(post_hoc['p-corr'])\n",
    "pvals_wanted = [pvals[i] for i in [38,43,50,65]]\n",
    "annotator = Annotator(ax3, pairs=pairs, data=s_rows, x='condition', y=dv, verbose=0)\n",
    "annotator.configure(test=None, text_format='star',loc='inside', verbose=0)\n",
    "annotator.set_pvalues_and_annotate(pvals_wanted)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir,'graphs/V1_violin_2mm'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statannotations.Annotator import Annotator  \n",
    "\n",
    "dv = 'delta_beta'\n",
    "\n",
    "# violin plots\n",
    "plot_data = df_with_factors_lgn[df_with_factors_lgn[\"condition\"] != 100]\n",
    "roi_rows = plot_data\n",
    "\n",
    "# per chromaticity\n",
    "lum_rows = roi_rows[roi_rows['colour'] == 'Lum']\n",
    "lm_rows = roi_rows[roi_rows['colour'] == 'LM']\n",
    "s_rows = roi_rows[roi_rows['colour'] == 'Scone']\n",
    "\n",
    "# plotting here\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(9, 5),sharey=True)\n",
    "\n",
    "# LUMINANCE\n",
    "palette = sns.color_palette('Greys', n_colors=2); palette.reverse()\n",
    "sns.violinplot(lum_rows, x='condition', y=dv, ax=ax1, \n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax1.set_xticks([0.5, 2.5]); ax1.set_xticklabels(['Disk','Grating'])\n",
    "ax1.set_xlabel(\" \")\n",
    "ax1.set_ylabel(\"Change in beta\")\n",
    "ax1.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax1.legend(title='Phase',loc='lower right')\n",
    "ax1.set_title('Luminance')\n",
    "ax1.spines['top'].set_visible(False); ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# L-M\n",
    "palette = sns.color_palette('Reds', n_colors=2); palette.reverse()\n",
    "sns.violinplot(lm_rows, x='condition', y=dv, ax=ax2,\n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax2.set_xticks([0.5, 2.5]); ax2.set_xticklabels(['Disk','Grating'])\n",
    "ax2.set_xlabel(\"Spatial Frequency\")\n",
    "ax2.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax2.legend(title='Phase',loc='lower right')\n",
    "ax2.set_title('L-M')\n",
    "ax2.spines['top'].set_visible(False); ax2.spines['right'].set_visible(False)\n",
    "\n",
    "# S\n",
    "palette = sns.color_palette('Blues', n_colors=2); palette.reverse()\n",
    "sns.violinplot(s_rows, x='condition', y=dv, ax=ax3,\n",
    "               hue='phase', split=True, inner='point',palette=palette)\n",
    "ax3.set_xticks([0.5, 2.5]); ax3.set_xticklabels(['Disk','Grating'])\n",
    "ax3.set_xlabel(\" \")\n",
    "ax3.axhline(0,color='grey', linestyle='--', linewidth=1)\n",
    "ax3.legend(title='Phase',loc='lower right')\n",
    "ax3.set_title('S-cone')\n",
    "ax3.spines['top'].set_visible(False); ax3.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_dir,'graphs/LGN_violin_2mm'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
